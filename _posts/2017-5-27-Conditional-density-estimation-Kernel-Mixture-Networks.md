---
layout: post
title: Kernel Mixture Networks
---

On a regular basis I feel like normal mean regression is not enough for use cases I am working on. Knowing about uncertainty of the reality that we are modeling and of the model itself can add a lot, especially in scenarios where decisions have to be made based on the output of your model. By predicting full probability distributions conditioned on your input (what will out output look like, given this specific input) we get a lot more information if done properly, and this distribution can always be collapsed into a mean again. 

Common use cases are:

- Monte Carlo sampling from (conditional) distribution
- Exploitation versus exploration dilemma in reinforcement learning
- Quantile regression without specifically training on one quantile
- Reversed quantile information, estimating the probability that the output will be lower or higher than a non-predetermined $y$
- Having non-linear costs after the output that require more knowledge than the expected value

In this blog post I will first look at common approaches to unconditional density estimation, then dive into some approaches to turn these into conditional density estimations and after that I will examine a freshly released paper called [The Kernel Mixture Network: A Nonparametric Method for Conditional Density Estimation of Continuous Random Variables](https://arxiv.org/abs/1705.07111) which uses a new combination to tackle this problem. Next to going through an implementation that my colleague and me made we will also look into some potential improvements on this paper.

## Unconditional density estimation

In the case of unconditional density estimation we are interested in estimating $p(\mathbf{x})$, a (joint) probability distribution over all the **n** dimensions. For this section we will look at increasing sets of points generated by a standard 1-dimensional Gaussian with $\mu=0$ and $\sigma=1$, see how the different estimations are affected by increasing sample sizes and what the downsides are of each method, before extending these to conditional density estimation techniques. 

### Histogram

A histogram discretizes your input dimension by using bins and counting the number of observations that fall within each bin. After normalizing the counts so that the surfaces of the bars with the bin widths and the normalized count as height sum to 1. Setting the bin width is a parameter of your distribution, as well as determining where the bins start and finish. There are numerous techniques and ideas for this which we will not go into, but I would like to show what sample size does with your histogram:

![histogram](/images/histogram.gif "Animated histogram")

As we can see in the animation, it takes a good number of samples to get close to the real distribution and because of the discretization we can never get really close without increasing to a high amount of bins. A bin width that is too small will mean the variance of the count will be very high which means it overfits on your data, while a bin width that is too big will throw away relevant information. An advantage of histograms is that it makes little assumptions with regards to what distribution generated your data and next to that it's a very intuitive way of visualizing a distribution. That said, getting the bin width right is tricky, using it to approximate a distribution means the range is bounded by the beginning and end of your bins and points near the borders of the bins might be closer to points in the neighboring bin than any point in it's own bin. The probability density can described by:

$$f(x) = \sum_{j=1}^k\mathfrak{U}_j(x)$$

Where $\mathfrak{U}_j(x)$ is the height of bin $j$ if $x$ falls within bin $j$, otherwise 0. 

### Fitting parameterized distribution

Instead of using a non-parametric approach like histograms or kernel density estimation we can use parametric distributions where we estimate the parameters to fit the assumed distribution. Depending on the amount of parameters, if the family of distributions fits the data well we can get very nice fits with relatively little amount of data, but if the family cannot fit the generating distribution properly you will also get very poor results. Some standard distributions like Gaussians are easy to fit by just looking at some statistics of your data, in case of the Gaussian the mean $\mu = \overline{X} = \frac{\sum_{i=1}^nx_i}{n}$ and $\overline\{\sigma}^2=\frac{\sum_{i=1}^n(x_i-\overline{X})^2}{n-1}$. In more complicated distributions you will need to use some form of iterative optimization like gradient descent on the likelihood function to fit your parameters. Here is a Gaussian fit on our data generated by a standard Gaussian, notice that this animation uses smaller sample sizes.

![parameterized](/images/normal_approximation.gif "Normal approximation")

Combining different distributions by weighing them allows for very flexible distributions. This means that when sampling from the complete distribution, we first sample from a discrete distribution that determines from which sub distribution we actually sample from. The full density is defined by:

$$f(x) = \sum_{j=1}^k\pi_jf_j(x)$$

Here, $\pi_j$ is the weight of subdistribution $j$, all $\pi_j\geq 0$ and $\sum_{j=1}^k\pi_j = 1$.

### Kernel density estimation

An alternative non-parametric option is to use kernel density estimation. The core concept is that every observation in our data contributes to a small subdistribution in our complete distribution. We do this by defining a distribution around each observation itself, using a kernel. A kernel is a function that is non-negative real-valued and in the case of non-parametric statistics preferably integrates to $1$ and is symmetric. There are many options here, empirically it is not very important, although you have to be careful that all your new observations will have a strictly positive likelihood like with the histogram. Regularly Gaussian kernels are used, these are defined as follows:

$$\mathcal{K}(x_1,x_2;\sigma)=\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x_1-x_2)^2}{2\sigma^2}}$$

We define this kernel for every of our observations and weight them with a factor $\frac{1}{n}$, which means the sum of these weighted kernels integrate to $1$ again. This makes our estimated density function:

$$f(x) = \frac{1}{n}\sum_{j=1}^n\mathcal{K}(x, x^{(j)})$$

The $\sigma$ determines the bandwidth of the kernel, which heavily influences the shape of our distribution. The bandwidth parameter is important, while there are some heuristics to calculate a good value, using cross validation with a number of bandwidths evaluated with the log likelihood is the preferred method of determining a good bandwidth. A bandwidth too small will generate a very wonky distribution while a bandwidth too big will lose a lot of its nuance in a similar way as the histogram does with bins that are too wide. Here we can see the effect of the bandwidth size on the shape of the distribution, from a sample size of 25:

![kernels](/images/kernels.gif "Kernel density estimation") 

## Conditional density estimation

Before we looked at the total data set $X$. Instead, we are now going to look for cases where we want to estimate a probability distribution over $y$, given our $\mathbf{x}$. Training and evaluating of these models usually involves the likelihood of our model $m$ given our data $(\mathbf{x}, y)$, which is equivalent to the density of our data given our model $m$. By looking at every sample and calculating the density of our function for that $y$ given our model and the corresponding features $\mathbf{x}$ and taking the product of each of these densities we arrive at the likelihood:

$$\mathcal{L}(m \mid \mathbf{x}, y) = P(y \mid \mathbf{x}, m) = \prod_{i=1}^n P(y_i\mid\mathbf{x_i}, m)$$

Due to numerical stability, usually the log likelihood is used, turning the product into a sum, which for optimization doesn't matter since the log function is monotonicly increasing. Since we usually talk about loss functions that we want to minimize with neural networks, we will also negate it and call it the negative log-likelihood:

$$-\log{(\mathcal{L}(m \mid \mathbf{x}, y))} = -\sum_{i=1}^n \log{(P(y_i\mid\mathbf{x_i}, m))}$$

When $ P(y_i \mid \mathbf{x_i}, m) $ is differentiable the negative log-likelihood is also differentiable, which means we can plug this into our favorite deep learning package and optimize our model directly, which is how the approaches we will discuss next work. All the output layers that we will talk about in the rest of this post can be put on top of any normal layer in a neural network, on top of a normal feed forward network but also on the hidden state of recurrent networks or after a convolutional layer.

### Quantized softmax networks

With a quantized softmax network, you estimate the height of each bar in a normalized histogram. You do this by chopping up the range of your output into **n** equally sized bins with a uniform density attached to each bin. The neural network will have **n** output nodes that have a softmax activation so that the output nodes sum to 1. Each output node represents the height of the corresponding bin, and given that the density integrated before weighing them, it still does because the bins are equal width and the weights still sum to 1. The density is now:

$$f(y|x) = \frac{\sum_{j=1}^k\mathfrak{U}_j(y)e^{z_j(\mathbf{x};W)}}{\sum_{j=1}^ke^{z_j(\mathbf{x};W)}}$$

Here $z_j$ represents output node $j$ given our input $\mathbf{x}$ and the trained parameters $W$. To train our network we will feed it pairs of samples and backpropagate our negative log-likelihood loss through the graph. The best the network can do is put all the weight on the bin that the $y$ belongs to, which means it will adapt $W$ to increase the weight there and decrease the weight at others. This approach is very intuitive but very effective. It is severely restrictive in the shape of the estimated distribution due to the discretization of the output space and your network will $NaN$ out when you have a $y$ that is outside of your bins, due to a $\log(0)$ in your loss function. That said, adding more bins only costs $m$ parameters per extra bin where $m$ is the size of the layer before the output layer.

### Mixture density networks

As we saw before, we can fit parameterized distributions using gradient descent. Instead of having these parameters fixed, we can also condition these based on our input. This is similar to estimating the weight distribution over the bins in the quantized softmax network approach. In case of predicting a Gaussian distribution given our input $x$, we would have a neural network with two outputs, one with an identity activation to represent $\mu(x)$ and one that needs a monotonically increasing strictly positive activation for $\sigma(x)$, for example the exponential activation $e^x$ or a softplus activation $\ln{(1+e^x)}$. Now we can optimize our network by minimizing the negative log-likelihood for our training examples. Problematic can be that if your model is expressive enough, it can heavily overfit by memorizing all the training samples and keep decreasing $\sigma$ to increase the density with $\mu$ exactly correct. Ways to combat this are regularization, either on your weights or directly punishing very low $\sigma$ outputs in your loss function, plus keeping track of your evaluation metrics and stopping whenever your likelihood starts getting worse. In this animation you can see a normal distribution fitting to one specific $y$ with the corresponding negative log-likelihood:

![normal_fitting](/images/normal_fitting.gif "Overfitting Normal Distribution") 

We can extend this idea by having a mixture of multiple distributions. Next to the direct parameters of all the distributions, we also need to determine the weights for all the distributions like with the quantized softmax networks. In the case of a mixture of **k** Gaussians we would have **3k** parameters to estimate. **k** $\sigma$, which are again transformed to be strictly positive, **k** $\mu$ that are just a linear combination of the previous layer and **k** $\pi$, a softmax distribution over the different distributions. You can also mix different distribution families in your mixture, however it is common practice to choose one, based on the range of your targets. Next to Gaussians, here are a few alternatives that you could use:

* * *

**Beta distribution**: 

Support is $x\in \left[0,1\right]$ or $x\in \left(0,1\right)$

Parameters: $\alpha$ and $\beta$. 

Density: $\frac{x^{\alpha-1}(1-x)^{\beta-1}}{B(\alpha,\beta)}$ with $B(\alpha,\beta)=\frac{\Gamma(\alpha)\Gamma(\beta)}{\Gamma(\alpha+\beta)}$.

Use case: modeling density over probabilities, fractions or other quantities in $\left[0,1\right]$.

* * *

**Gamma distribution**: 

Support: $x\in \left(0,\infty\right)$. 

It has two common parameterizations, we will use the Bayesian one with shape $\alpha$ and rate $\beta$. 

Density: $\frac{\beta^\alpha}{\Gamma(\alpha)}x^{\alpha-1}e^{-\beta x}$ 

Use case: commonly used for approximating distributions over strictly positive values. Another option for this range could be the Weibull distribution.

* * *

Of course these distributions are easy to shift. Any parametric distribution that has a differentiable density function can be used in this way, however be aware that training these can be difficult. Initialization can be tricky and sometimes has to be done again when the likelihood is $0$ for certain points. In that regard, make sure to normalize your targets to zero mean and unit variance, so that the expectation of your initialization is also $0$ (initialize all your biases at 0). Even when the initialization is working, sometimes the network will get in a part of the parameter space where one of your training samples will suddenly fall outside of a positive density and break your network.

## Kernel Mixture Networks
In the paper that we linked to the authors combine the approach of unconditional kernel density and quantized softmax networks to get to a conditional kernel density. The idea is fairly simple but elegant. Just like with unconditional kernel density, we use all the targets of our training samples or a downsampled amount as kernels for our distribution. However, instead of weighing each one of them uniformly, we want to redistribute the weight conditioned on our input. This is basically the same as we do with the quantized softmax networks, however we use kernel densities instead of discretized bins. In this animation you can see all the weight slowly going towards one of the kernels:

![kernels_shift](/images/kernels_shift.gif "Kernel shift towards one") 

And in this one we move all the weights equally to two different kernels to go towards a bimodal distribution:

![kernels_shift_bimodal](/images/kernels_shift_bimodal.gif "Kernel shift towards two modes") 

The output layer again is a softmax layer that indicates the weight for each kernel. This allows for  The kernels itself don't even need to be properly differentiable because we only adjust the weights of the kernels. First we will look at picking the kernels and bandwidths, after that we will also look at our implementation in TensorFlow, Edward + Keras and potential improvements for this method.

### Kernels and bandwidths

The kernels form the bases for our densities. While the shape of our kernels might not be too relevant, the bandwidth determines a lot about the shape of our output density. If the bandwidth is too small, spikes show up in the densities that are typical of overfitting. Corresponding to this your evaluation likelihood will become poor because targets that are a bit further from the kernel centers will have a terrible likelihood. On the other hand, if your bandwidth is too wide you lose information, the distributions will have a very generic shape and your likelihood will be bad because nowhere are high densities. The bandwidth to use is a hyperparameter of the model, as it is described in the paper. We will get back to this later.

In small data scenarios using all the targets as kernel centers is not a bad idea, however there will be diminishing returns with bigger data sets, due to highly similar targets. The network has no use for duplicate kernels while it does increase the degrees of freedom of the network and decrease performance. The paper removes a kernel center when it's very close to another kernel center. Our suggestion is to use clustering on your target space instead, basically averaging centers that are close together. This idea is inspired by Radial Basis Function networks that were used in the past, they initialize their radial kernels in a similar fashion. There are other similarities between the approaches, the big difference is that RBF networks use these kernels as feature extractors while Kernel Mixture networks use the kernels to parameterize the output distribution. If you cluster to a high number of centers, another suggestion that we have not actually tested is to keep the edges as centers, because your model cannot express modes that are outside the centers on the edges, kind of reducing the range of your network if you throw away the edge cases. With a low number of centers this is probably not worth it. Our implementation includes the 'use all', 'k-means' and 'agglomerative' clustering approaches to get a list of kernel centers to use. In the case of 1-dimensional clustering, there is a better alternative for k-means like [Jenks natural breaks optimization](https://en.wikipedia.org/wiki/Jenks_natural_breaks_optimization) but we used k-means because it doesn't matter too much and sci-kit learn has an easy API.

Using one bandwidth allows for small or wide distributions by putting all the weight in one kernel or spreading the weights over multiple kernels. Instead of having one bandwidth we could also have multiple kernels per center, each with their own bandwidth. This allows for adjusting the spread around one center, based on our input. Here you can see the effect of shifting the weights around one center:

ANIMATIE

Now we will diverge a bit from the paper. There is no real reason to keep the $\sigma$ fixed. Using backpropagation in our networks, we can adjust not just the weights in our network that parameterize the weights over our kernels, but also adjust our $\sigma$. When $\sigma$ becomes too small, the likelihood will get poor because of samples that have a very small density because they are a bit too far from kernels while the likelihood will also be poor with a bandwidth that is too high, because the density of the kernels is too flat. This is something that a network can learn itself very well, and our experiments show this (TODO). Extending this too multiple bandwidths does not work as well as we hoped. What we see is that the list of different $\sigma$ almost converges to the same value. We believe this is due to the fact that for a training batch, all the $\sigma$ will be pulled in the same direction. There are some ways to combat this problem, you could add a penalty term to your loss function that punishes low variance in your $\sigma$ vector which means the network is incentivized to have more different values. Another way is to have one trainable $\sigma$ and use for example $\left[\sigma, 2\sigma, 3\sigma, 4\sigma\right]$ as bandwidths. We have not tested these options yet but they are easy to implement.

### Implementation

We have [implemented](https://github.com/janvdvegt/KMN/blob/master/src/kmn.py) the paper and ideas in a class consiting of a combination of TensorFlow, Edward and Keras, where you can easily plugin your own network and data and add a Kernel Mixture Network output layer to output conditional distributions over your target. Here we will look at an implementation in TensorFlow which is a bit lower level, to get a better understanding of how it works. We will use k-means clustering from sci-kit learn for finding the centers and use Gaussian kernels with multiple, non-trainable bandwidths.

First we need to cluster the targets, here is an easy routine for 'k-means:

```python
import numpy as np
from sklearn.cluster import KMeans

def kmeans_centers(y, k=100):
    kmeans = KMeans(n_clusters=k, n_jobs=-2)
    kmeans.fit(np.array(train_y).reshape(-1, 1))
    cluster_centers = kmeans.cluster_centers_
    return cluster_centers.ravel()
```

Next, we need a Gaussian kernel function that calculates the kernel value between two values, during training this will be between the observed $y$ and kernel center $y^{(p)}$. This will be a TensorFlow node in our graph that outputs a new tensor. The kernel function is:

$$\mathcal{K}(y,y';\sigma)=\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(y-y')^2}{2\sigma^2}}$$

```python
from math import pi
import tensorflow as tf

def gaussian_kernel(y, y_acc, sigma):
    return 1. / (tf.sqrt(2 * pi) * sigma * tf.exp(-tf.square(y - y_acc) / (2 * tf.square(sigma))
```

Now we want to define the final output layer that takes the weight tensor, the target $y$, a list of kernel centers and a sigma and that will return the density for the passed $y$, given the weights.

```python
def density_output_layer(w, y, cluster_centers, sigma):
    output_kernels = []
    for center in cluster_centers:
        output_kernels.append(tf.reshape(gaussian_kernel(y, center, sigma), [-1, 1]))
    output_kernels = tf.concat(output_kernels, axis=1)
    output_nodes = tf.multiply(w, output_kernels)
    return tf.reduce_sum(output_nodes, axis=1) / total_weight
```

This layer takes in the second to last layer of the network which has a form of a softmax output to make sure the density still sums to one. After wiring the inputs, normal layers, the softmax weight layer and the density output layer together we can put in samples $x$ and a $y$ and the network returns the density of this specific $y$ given our input $x$. To train the network we need a loss function however, fortunately the negative log-likelihood is easy once we have access to the density. Here is the function:

$$-\log{(\mathcal{L}(m \mid \mathbf{x}, y))} = -\sum_{i=1}^n \log{(P(y_i\mid\mathbf{x_i}, m))}$$

In this case $n$ is the size of our training batch. If our density output layer is called 'density', our loss function is defined as follows:

```python
loss = -tf.log(density)
mean_loss = tf.reduce_mean(loss)
train = tf.train.AdamOptimizer().minimize(mean_loss)
```

Once we have our train operation in our TensorFlow graph we can pass batches and optimize our network. If we want to sample from a conditional distribution, we can first sample from our weights output which is a discrete probability distribution, and then from the corresponding kernel. Our class made using Edward has a clean sampling method baked in.

### More dimensions

One last thing I would like to discuss is that this concept is very easy to extend to multiple output dimensions. Instead of using 1-dimensional Gaussian kernels, we can also use **n**-dimensional Gaussian kernels, by using the squared norm between the $\mathbf{y}$ and $\mathbf{y}'$ output vectors. This turns our Kernel function into:

$$\mathcal{K}(\mathbf{y},\mathbf{y}';\sigma)=\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{\left\Vert \mathbf{y}-\mathbf{y}'\right\Vert^2}{2\sigma^2}}$$

